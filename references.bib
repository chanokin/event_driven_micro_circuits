@inproceedings{sacramento,
 author = {Sacramento, Jo\~{a}o and Ponte Costa, Rui and Bengio, Yoshua and Senn, Walter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Dendritic cortical microcircuits approximate the backpropagation algorithm},
 url = {https://proceedings.neurips.cc/paper/2018/file/1dc3a89d0d440ba31729b0ba74b93a33-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{bengio_bio_deep,
  author    = {Yoshua Bengio and
               Dong{-}Hyun Lee and
               J{\"{o}}rg Bornschein and
               Zhouhan Lin},
  title     = {Towards Biologically Plausible Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1502.04156},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.04156},
  eprinttype = {arXiv},
  eprint    = {1502.04156},
  timestamp = {Mon, 13 Aug 2018 16:46:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BengioLBL15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cartiglia_snn_prop,
  author    = {Matteo Cartiglia and
               Germain Haessig and
               Giacomo Indiveri},
  title     = {An error-propagation spiking neural network compatible with neuromorphic
               processors},
  journal   = {CoRR},
  volume    = {abs/2104.05241},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.05241},
  eprinttype = {arXiv},
  eprint    = {2104.05241},
  timestamp = {Mon, 19 Apr 2021 16:45:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-05241.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{lee_diff_prop,
      title={Difference Target Propagation}, 
      author={Dong-Hyun Lee and Saizheng Zhang and Asja Fischer and Yoshua Bengio},
      year={2015},
      eprint={1412.7525},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{genn,
  title={GeNN: a code generation framework for accelerated brain simulations},
  author={Yavuz, Esin and Turner, James and Nowotny, Thomas},
  journal={Scientific reports},
  volume={6},
  number={1},
  pages={1--14},
  year={2016},
  publisher={Nature Publishing Group}
}


@ARTICLE{pygenn,
AUTHOR={Knight, James C. and Komissarov, Anton and Nowotny, Thomas},   
TITLE={PyGeNN: A Python Library for GPU-Enhanced Neural Networks},      
JOURNAL={Frontiers in Neuroinformatics},      
VOLUME={15},      
YEAR={2021},      
URL={https://www.frontiersin.org/article/10.3389/fninf.2021.659005},       
DOI={10.3389/fninf.2021.659005},      
ISSN={1662-5196},   
}

@book{deep-learning-book,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{deep-learning-nature,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

﻿@Article{backprop,
author={Rumelhart, David E.
and Hinton, Geoffrey E.
and Williams, Ronald J.},
title={Learning representations by back-propagating errors},
journal={Nature},
year={1986},
month={Oct},
day={01},
volume={323},
number={6088},
pages={533-536},
abstract={We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
issn={1476-4687},
doi={10.1038/323533a0},
url={https://doi.org/10.1038/323533a0}
}

@ARTICLE{stdp,
AUTHOR = {Sjöström, J.  and Gerstner, W. },
TITLE   = {{S}pike-timing dependent plasticity},
YEAR    = {2010},
JOURNAL = {Scholarpedia},
VOLUME  = {5},
NUMBER  = {2},
PAGES   = {1362},
DOI     = {10.4249/scholarpedia.1362},
NOTE    = {revision \#184913}
}

@article{target-prop,
  author    = {Yoshua Bengio},
  title     = {How Auto-Encoders Could Provide Credit Assignment in Deep Networks
               via Target Propagation},
  journal   = {CoRR},
  volume    = {abs/1407.7906},
  year      = {2014},
  url       = {http://arxiv.org/abs/1407.7906},
  eprinttype = {arXiv},
  eprint    = {1407.7906},
  timestamp = {Mon, 13 Aug 2018 16:48:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Bengio14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{urbanczik,
title = {Learning by the Dendritic Prediction of Somatic Spiking},
journal = {Neuron},
volume = {81},
number = {3},
pages = {521-528},
year = {2014},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2013.11.030},
url = {https://www.sciencedirect.com/science/article/pii/S0896627313011276},
author = {Robert Urbanczik and Walter Senn},
abstract = {Summary
Recent modeling of spike-timing-dependent plasticity indicates that plasticity involves as a third factor a local dendritic potential, besides pre- and postsynaptic firing times. We present a simple compartmental neuron model together with a non-Hebbian, biologically plausible learning rule for dendritic synapses where plasticity is modulated by these three factors. In functional terms, the rule seeks to minimize discrepancies between somatic firings and a local dendritic potential. Such prediction errors can arise in our model from stochastic fluctuations as well as from synaptic input, which directly targets the soma. Depending on the nature of this direct input, our plasticity rule subserves supervised or unsupervised learning. When a reward signal modulates the learning rate, reinforcement learning results. Hence a single plasticity rule supports diverse learning paradigms.}
}

@article{yinyang_dataset,
  author    = {Laura Kriener and
               Julian G{\"{o}}ltz and
               Mihai A. Petrovici},
  title     = {The Yin-Yang dataset},
  journal   = {CoRR},
  volume    = {abs/2102.08211},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.08211},
  eprinttype = {arXiv},
  eprint    = {2102.08211},
  timestamp = {Fri, 19 Feb 2021 11:02:14 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-08211.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
